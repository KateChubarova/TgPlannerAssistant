import json

from langsmith import traceable
from openai.types.chat import ChatCompletionMessageFunctionToolCall

from rag.open_ai_client import CHAT_MODEL, openai_client, system_prompt
from rag.tools.company_info_tool import company_info_tool
from rag.tools.location_tool import location_tool
from shared.models.embedding import Embedding
from shared.models.user import TgUser
from shared.nlp.embeddings import embed_query
from shared.storage.embeddings_repo import search_similar_embeddings
from sources.web_search.client import enrich_company_info, enrich_event_by_location


def build_context(records: list[Embedding]) -> str:
    """
    Build a textual context from a list of calendar embeddings.

    This function formats each embedding record into a human-readable string
    containing the calendar name, source, combined text, and participants,
    then joins them into a single context block.

    Args:
        records (list[Embedding]): A list of embedding records representing calendar events.

    Return:
        str: A formatted context string built from the records, or a fallback
            message if no relevant calendar entries are available.
    """
    parts = []
    for record in records:
        parts.append(f"{record.combined_text}")

    return "\n".join(parts) if parts else "Нет релевантных записей календаря."


@traceable(run_type="llm")
def answer_with_rag(
    user: TgUser,
    user_query: str,
    embed_fn=embed_query,
    search_fn=search_similar_embeddings,
    top_k=5,
) -> str | None:
    """
    Generate an answer using a retrieval-augmented generation (RAG) pipeline.

    This function embeds the user's query, searches for similar calendar
    embeddings, builds a context from them, and calls the chat model with
    optional tool usage. If a location-related tool is invoked, the response
    is further enriched with location information.

    Args:
        user (TgUser): The Telegram user object whose data and embeddings are used.
        user_query (str): The original text query submitted by the user.
        top_k (int): The maximum number of similar embeddings to retrieve for context.

    Return:
        str: The final answer generated by the chat model, or None if
            no answer could be produced.
    """
    query_embedding = embed_fn(user_query)
    rows = search_fn(user, query_embedding, top_k=top_k)

    context = build_context(rows)

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "assistant", "content": f"Контекст событий пользователя:\n{context}"},
        {"role": "user", "content": user_query},
    ]

    completion = openai_client.chat.completions.create(
        model=CHAT_MODEL,
        messages=messages,
        tools=[location_tool, company_info_tool],
        tool_choice="auto",
    )

    answer = completion.choices[0].message
    messages.append(answer)

    if answer.tool_calls:
        for tool_call in answer.tool_calls:
            fn_name = tool_call.function.name
            if fn_name == "enrich_event_by_location":
                return answer_with_location_info(tool_call, messages)
            if fn_name == "enrich_company_info":
                return answer_with_company_info(tool_call, messages)
    else:
        return answer.content


@traceable(run_type="llm")
def answer_with_location_info(
    tool_call: ChatCompletionMessageFunctionToolCall, messages: [dict]
) -> str:
    """
    Handle a location tool call and enrich the answer with location-based information.

    This function parses the tool call arguments to extract the location,
    fetches additional data for that location, appends the tool result to
    the messages, and then calls the chat model again to generate a final,
    location-aware answer.

    Args:
        tool_call (ChatCompletionMessage): The tool call message containing the
            function name and JSON arguments from the model.
        messages (list[dict]): The list of chat messages representing the current
            conversation state, including system, user, assistant, and tool turns.

    Return:
        str: The final answer text generated by the chat model after incorporating
            the location enrichment result.
    """
    args = json.loads(tool_call.function.arguments)
    location = args["location"]

    tool_result = enrich_event_by_location(location=location)
    messages.append(
        {
            "role": "tool",
            "tool_call_id": tool_call.id,
            "name": "enrich_event_by_location",
            "content": json.dumps(tool_result, ensure_ascii=False),
        }
    )

    final = openai_client.chat.completions.create(
        model=CHAT_MODEL,
        messages=messages,
    )

    return final.choices[0].message.content


@traceable(run_type="llm")
def answer_with_company_info(
    tool_call: ChatCompletionMessageFunctionToolCall,
    messages: list[dict],
) -> str:
    """
    Handle a company info tool call and generate a final model response.

    This function:
        1. Parses the tool call arguments to extract the company name.
        2. Calls `enrich_company_info` to fetch additional company information.
        3. Appends the tool result to the conversation messages.
        4. Performs a follow-up model call to produce the final user-facing answer.

    Args:
        tool_call (ChatCompletionMessageFunctionToolCall):
            The tool call message containing the function name and JSON arguments.
        messages (list[dict]):
            The current conversation message history including system, user,
            assistant, and tool messages.

    Return:
        str: The final response text generated by the model after incorporating
                the enriched company information.
    """
    args = json.loads(tool_call.function.arguments or "{}")

    company_name = args.get("company_name")

    tool_result = enrich_company_info(
        company_name=company_name,
    )
    messages.append(
        {
            "role": "tool",
            "tool_call_id": tool_call.id,
            "name": "enrich_company_info",
            "content": json.dumps(tool_result, ensure_ascii=False),
        }
    )

    final = openai_client.chat.completions.create(
        model=CHAT_MODEL,
        messages=messages,
    )

    return final.choices[0].message.content
